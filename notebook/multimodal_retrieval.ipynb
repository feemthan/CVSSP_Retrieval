{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "import clip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# embeddings: https://deploy.laion.ai/8f83b608504d46bb81708ec86e912220/embeddings/img_emb/img_emb_0.npy\n",
    "# parquet: https://deploy.laion.ai/8f83b608504d46bb81708ec86e912220/embeddings/metadata/metadata_0.parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aria2c https://deploy.laion.ai/8f83b608504d46bb81708ec86e912220/embeddings/img_emb/img_emb_0.npy\n",
    "# !aria2c https://deploy.laion.ai/8f83b608504d46bb81708ec86e912220/embeddings/metadata/metadata_0.parquet\n",
    "# !aria2c https://deploy.laion.ai/8f83b608504d46bb81708ec86e912220/embeddings/text_emb/text_emb_0.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000448, 512) (1000448, 512)\n"
     ]
    }
   ],
   "source": [
    "img_emb = np.load('multimodal_embeddings/img_emb/img_emb_0.npy')\n",
    "text_emb = np.load('multimodal_embeddings/text_emb/text_emb_0.npy')\n",
    "print(img_emb.shape, text_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elementwise combination for multimodal embeddings\n",
    "w1 = 0.5\n",
    "w2 = 0.5\n",
    "\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2 == 0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "multimodal_emb = normalized(w1*img_emb + w2*text_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('multimodal_embeddings/combined_emb/combined_emb.npy', multimodal_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:34:56,113 [INFO]: Using 12 omp threads (processes), consider increasing --nb_cores if you have more\n",
      "2024-07-08 20:34:56,113 [INFO]: Launching the whole pipeline 07/08/2024, 20:34:56\n",
      "2024-07-08 20:34:56,113 [INFO]: Reading total number of vectors and dimension 07/08/2024, 20:34:56\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 6921.29it/s]\n",
      "2024-07-08 20:34:56,132 [INFO]: There are 1000448 embeddings of dim 512\n",
      "2024-07-08 20:34:56,132 [INFO]: >>> Finished \"Reading total number of vectors and dimension\" in 0.0191 secs\n",
      "2024-07-08 20:34:56,132 [INFO]: \tCompute estimated construction time of the index 07/08/2024, 20:34:56\n",
      "2024-07-08 20:34:56,132 [INFO]: \t\t-> Train: 16.7 minutes\n",
      "2024-07-08 20:34:56,132 [INFO]: \t\t-> Add: 5.7 seconds\n",
      "2024-07-08 20:34:56,133 [INFO]: \t\tTotal: 16.8 minutes\n",
      "2024-07-08 20:34:56,133 [INFO]: \t>>> Finished \"Compute estimated construction time of the index\" in 0.0002 secs\n",
      "2024-07-08 20:34:56,133 [INFO]: \tChecking that your have enough memory available to create the index 07/08/2024, 20:34:56\n",
      "2024-07-08 20:34:56,133 [INFO]: 2.4GB of memory will be needed to build the index (more might be used if you have more)\n",
      "2024-07-08 20:34:56,133 [INFO]: \t>>> Finished \"Checking that your have enough memory available to create the index\" in 0.0005 secs\n",
      "2024-07-08 20:34:56,133 [INFO]: \tSelecting most promising index types given data characteristics 07/08/2024, 20:34:56\n",
      "2024-07-08 20:34:56,133 [INFO]: \t>>> Finished \"Selecting most promising index types given data characteristics\" in 0.0000 secs\n",
      "2024-07-08 20:34:56,133 [INFO]: \tCreating the index 07/08/2024, 20:34:56\n",
      "2024-07-08 20:34:56,134 [INFO]: \t\t-> Instanciate the index HNSW32 07/08/2024, 20:34:56\n",
      "2024-07-08 20:34:56,134 [INFO]: \t\t>>> Finished \"-> Instanciate the index HNSW32\" in 0.0001 secs\n",
      "2024-07-08 20:34:56,134 [INFO]: \t\t-> Adding the vectors to the index 07/08/2024, 20:34:56\n",
      "2024-07-08 20:34:56,134 [INFO]: The memory available for adding the vectors is 29.9GB(total available - used by the index)\n",
      "2024-07-08 20:34:56,134 [INFO]: Using a batch size of 488281 (memory overhead 953.7MB)\n",
      "100%|███████████████████████████████████████████| 23/23 [01:28<00:00,  3.86s/it]\n",
      "2024-07-08 20:36:24,834 [INFO]: \tComputing best hyperparameters for index combined.index 07/08/2024, 20:36:24\n",
      "2024-07-08 20:36:40,126 [INFO]: \t>>> Finished \"Computing best hyperparameters for index combined.index\" in 15.2925 secs\n",
      "2024-07-08 20:36:40,127 [INFO]: The best hyperparameters are: efSearch=1751\n",
      "2024-07-08 20:36:40,127 [INFO]: \tCompute fast metrics 07/08/2024, 20:36:40\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]\n",
      "2024-07-08 20:36:56,250 [INFO]: \t>>> Finished \"Compute fast metrics\" in 16.1228 secs\n",
      "2024-07-08 20:36:56,250 [INFO]: \tSaving the index on local disk 07/08/2024, 20:36:56\n",
      "2024-07-08 20:37:02,490 [INFO]: \t>>> Finished \"Saving the index on local disk\" in 6.2399 secs\n",
      "2024-07-08 20:37:02,491 [INFO]: \t\t>>> Finished \"-> Adding the vectors to the index\" in 126.3570 secs\n",
      "2024-07-08 20:37:02,491 [INFO]: {\n",
      "2024-07-08 20:37:02,491 [INFO]: \tindex_key: HNSW32\n",
      "2024-07-08 20:37:02,491 [INFO]: \tindex_param: efSearch=1751\n",
      "2024-07-08 20:37:02,491 [INFO]: \tindex_path: combined.index\n",
      "2024-07-08 20:37:02,491 [INFO]: \tsize in bytes: 2321169762\n",
      "2024-07-08 20:37:02,491 [INFO]: \tavg_search_speed_ms: 8.519120848702325\n",
      "2024-07-08 20:37:02,492 [INFO]: \t99p_search_speed_ms: 14.878794981632382\n",
      "2024-07-08 20:37:02,492 [INFO]: \treconstruction error %: 0.0\n",
      "2024-07-08 20:37:02,492 [INFO]: \tnb vectors: 1000448\n",
      "2024-07-08 20:37:02,492 [INFO]: \tvectors dimension: 512\n",
      "2024-07-08 20:37:02,492 [INFO]: \tcompression ratio: 0.8827090278112971\n",
      "2024-07-08 20:37:02,492 [INFO]: }\n",
      "2024-07-08 20:37:02,492 [INFO]: \t>>> Finished \"Creating the index\" in 126.3586 secs\n",
      "2024-07-08 20:37:02,492 [INFO]: >>> Finished \"Launching the whole pipeline\" in 126.3791 secs\n",
      "(<faiss.swigfaiss_avx2.IndexHNSWFlat; proxy of <Swig Object of type 'faiss::IndexHNSWFlat *' at 0x74343c6587e0> >, {'index_key': 'HNSW32', 'index_param': 'efSearch=1751', 'index_path': 'combined.index', 'size in bytes': 2321169762, 'avg_search_speed_ms': 8.519120848702325, '99p_search_speed_ms': 14.878794981632382, 'reconstruction error %': 0.0, 'nb vectors': 1000448, 'vectors dimension': 512, 'compression ratio': 0.8827090278112971})\n"
     ]
    }
   ],
   "source": [
    "## Build sample Index\n",
    "!autofaiss build_index --embeddings=\"multimodal_embeddings/combined_emb\" \\\n",
    "                    --index_path=\"combined.index\" \\\n",
    "                    # --index_infos_path=\"infos.json\" \\\n",
    "                    --metric_type=\"ip\" \\\n",
    "                    --max_index_query_time_ms=5 \n",
    "                    # \\\n",
    "                    # --max_index_memory_usage=\"1GB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "# check this for text and do a compare between text and image combined\n",
    "img_ind = faiss.read_index(\"img.index\")\n",
    "text_ind = faiss.read_index(\"text.index\")\n",
    "combined_ind = faiss.read_index(\"combined.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "data_dir = Path(\"embeddings\")\n",
    "df = pd.concat(\n",
    "    pd.read_parquet(parquet_file)\n",
    "    for parquet_file in data_dir.glob('*.parquet')\n",
    ")\n",
    "print(df.head(2))\n",
    "image_list = df[\"image_path\"].tolist()\n",
    "caption_list = df[\"caption\"].tolist()\n",
    "url_list = df[\"url\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text to Text query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Black kitten walking\"\n",
    "text_tokens = clip.tokenize([text], truncate=True)\n",
    "\n",
    "text_features = model.encode_text(text_tokens.to(device))\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "text_embeddings = text_features.cpu().detach().numpy().astype('float32')\n",
    "\n",
    "print(\"=\"*10)\n",
    "print(\"text query = {}\".format(text))\n",
    "\n",
    "D, I = text_ind.search(text_embeddings, 5)\n",
    "print(\"results :\")\n",
    "for d, i in zip(D[0], I[0]):\n",
    "  print(\"Similarity= \", d)\n",
    "  print(\"Index ={}\".format(i))\n",
    "  print(\"Caption ={}\".format(caption_list[i]))\n",
    "  print(\"Image url = {}\".format(url_list[i]))\n",
    "  print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text to Image query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Black kitten walking\"\n",
    "text_tokens = clip.tokenize([text], truncate=True)\n",
    "\n",
    "text_features = model.encode_text(text_tokens.to(device))\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "text_embeddings = text_features.cpu().detach().numpy().astype('float32')\n",
    "\n",
    "print(\"=\"*10)\n",
    "print(\"text query = {}\".format(text))\n",
    "\n",
    "D, I = img_ind.search(text_embeddings, 5)\n",
    "print(\"results :\")\n",
    "for d, i in zip(D[0], I[0]):\n",
    "  print(\"Similarity= \", d)\n",
    "  print(\"Index ={}\".format(i))\n",
    "  print(\"Caption ={}\".format(caption_list[i]))\n",
    "  print(\"Image url = {}\".format(url_list[i]))\n",
    "  print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image to Image query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = Image.open(\"newcat.jpg\")\n",
    "image_tensor = preprocess(image)\n",
    "\n",
    "image_features = model.encode_image(torch.unsqueeze(image_tensor.to(device), dim=0))\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "image_embeddings = image_features.cpu().detach().numpy().astype('float32')\n",
    "\n",
    "print(\"query :\")\n",
    "display(image) \n",
    "\n",
    "D, I = img_ind.search(image_embeddings, 5)\n",
    "print(\"results :\")\n",
    "for d, i in zip(D[0], I[0]):\n",
    "  print(\"Similarity= \", d)\n",
    "  print(\"Index ={}\".format(i))\n",
    "  print(\"Caption ={}\".format(caption_list[i]) )\n",
    "  print(\"Image url = {}\".format(url_list[i]))\n",
    "  print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image to text query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = Image.open(\"newcat.jpg\")\n",
    "image_tensor = preprocess(image)\n",
    "\n",
    "image_features = model.encode_image(torch.unsqueeze(image_tensor.to(device), dim=0))\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "image_embeddings = image_features.cpu().detach().numpy().astype('float32')\n",
    "\n",
    "print(\"query :\")\n",
    "display(image) \n",
    "\n",
    "D, I = text_ind.search(image_embeddings, 5)\n",
    "print(\"results :\")\n",
    "for d, i in zip(D[0], I[0]):\n",
    "  print(\"Similarity= \", d)\n",
    "  print(\"Index ={}\".format(i))\n",
    "  print(\"Caption ={}\".format(caption_list[i]) )\n",
    "  print(\"Image url = {}\".format(url_list[i]))\n",
    "  print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi modal Image-Text query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = Image.open(\"newcat.jpg\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "image_tensor = preprocess(image)\n",
    "\n",
    "image_features = model.encode_image(torch.unsqueeze(image_tensor.to(device), dim=0))\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "image_embeddings = image_features.cpu().detach().numpy().astype('float32')\n",
    "\n",
    "print(\"=\"*10)\n",
    "print(\"query :\")\n",
    "display(image) \n",
    "\n",
    "text = \"black cat with yellow eyes\"\n",
    "text_tokens = clip.tokenize([text], truncate=True)\n",
    "\n",
    "text_features = model.encode_text(text_tokens.to(device))\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "text_embeddings = text_features.cpu().detach().numpy().astype('float32')\n",
    "\n",
    "print(\"=\"*10)\n",
    "print(\"text query = {}\".format(text))\n",
    "\n",
    "combined_embeddings = 0.3 * image_embeddings + 0.7 * text_embeddings\n",
    "\n",
    "D, I = combined_ind.search(combined_embeddings, 5)\n",
    "print(\"results :\")\n",
    "for d, i in zip(D[0], I[0]):\n",
    "  print(\"Similarity= \", d)\n",
    "  print(\"Index ={}\".format(i))\n",
    "  print(\"Caption ={}\".format(caption_list[i]) )\n",
    "  print(\"Image url = {}\".format(url_list[i]))\n",
    "  print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
